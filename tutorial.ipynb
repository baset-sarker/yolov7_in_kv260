{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9f7373-53c6-47e4-b306-8a9f7653abb1",
   "metadata": {},
   "source": [
    "This YOLOv7 ðŸš€ notebook by Vitis AI presents simple train, validate and predict examples to help start your AI adventure.\n",
    "We hope that the resources in this notebook will help you get the most out of YOLOv7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f241107",
   "metadata": {},
   "source": [
    "# Setup and Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d6162-9ebd-4b22-a501-a6d069c3d4cc",
   "metadata": {},
   "source": [
    "Clone GitHub repository, install dependencies and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f5d64-e171-48b6-8ae1-895d3c28edde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r yolov7/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\n",
    "!sudo apt install -y g++-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.23.4\n",
    "!pip install pandas==1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2b49c-4a87-46dd-9f70-c1882c45098e",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141f90a-2b7f-4d1f-a11e-1e288137c7b0",
   "metadata": {},
   "source": [
    "##### Download COCO2017 dataset.(refer to this repo https://github.com/WongKinYiu/yolov7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1d0c0-2c67-4533-82dd-772f1478055a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd yolov7/\n",
    "!bash scripts/get_coco.sh\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf86ac7-7152-4574-a819-8d6f719948e1",
   "metadata": {},
   "source": [
    "### After download the coco dataset, the directory structure should be:\n",
    "```markdown\n",
    "+ yolov7/\n",
    "    + coco/\n",
    "        + labels/\n",
    "        + annotations/\n",
    "        + images/\n",
    "        + test-dev2017.txt \n",
    "        + train2017.txt\n",
    "        + val2017.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230681a",
   "metadata": {},
   "source": [
    "## Eval  Post-training quanzization model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b235516",
   "metadata": {},
   "source": [
    "## Quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92fefb6-598a-438a-866f-2cb9b374788d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/internal-models/pytorch/yolov7\n"
     ]
    }
   ],
   "source": [
    "%cd yolov7/\n",
    "!python -m torch.distributed.launch --nproc_per_node 4 --master_port 9004 train_qat.py --workers 8 --device 0,1,2,3 --epochs 20 --batch-size 32 --data data/coco.yaml --img 640 640 --cfg cfg/training/yolov7.yaml --weights yolov7.pt --name yolov7_qat --hyp data/hyp.scratch.p5_qat.yaml --nndct_convert_sigmoid_to_hsigmoid --nndct_convert_silu_to_hswish --log_threshold_scale 100\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train yolov7 tiny model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699ac8f2-aaa6-4b4e-9a9d-e5b8c385d670",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/yolov7/yolov7\n",
      "YOLOR ðŸš€ v3.5-19-gb2b227921 torch 1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3090 Ti, 24239.5625MB)\n",
      "\n",
      "Namespace(adam=False, artifact_alias='latest', batch_size=8, bbox_interval=-1, bucket='', cache_images=False, cfg='cfg/training/yolov7-tiny.yaml', data='data/mp.yaml', device='0', entity=None, epochs=100, evolve=False, exist_ok=False, freeze=[0], global_rank=-1, hyp='data/hyp.scratch.p5_qat.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, log_threshold_scale=100, multi_scale=False, name='yolov7-mp-aug5-tiny-qat', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/yolov7-mp-aug5-tiny-qat2', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=8, upload_dataset=False, v5_metric=False, weights='yolov7-tiny.pt', workers=16, world_size=1)\n",
      "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0001, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n",
      "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  2                -1  1      2112  models.common.Conv                      [64, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  3                -2  1      2112  models.common.Conv                      [64, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  4                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  5                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "  6  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      "  7                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  8                -1  1         0  models.common.MP                        []                            \n",
      "  9                -1  1      4224  models.common.Conv                      [64, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 10                -2  1      4224  models.common.Conv                      [64, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 11                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 12                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 13  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 15                -1  1         0  models.common.MP                        []                            \n",
      " 16                -1  1     16640  models.common.Conv                      [128, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 17                -2  1     16640  models.common.Conv                      [128, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 20  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 21                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 22                -1  1         0  models.common.MP                        []                            \n",
      " 23                -1  1     66048  models.common.Conv                      [256, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 24                -2  1     66048  models.common.Conv                      [256, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 25                -1  1    590336  models.common.Conv                      [256, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 26                -1  1    590336  models.common.Conv                      [256, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 27  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 28                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 29                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 30                -2  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 31                -1  1         0  models.common.SP                        [5]                           \n",
      " 32                -2  1         0  models.common.SP                        [9]                           \n",
      " 33                -3  1         0  models.common.SP                        [13]                          \n",
      " 34  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 35                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 36          [-1, -7]  1         0  models.common.Concat                    [1]                           \n",
      " 37                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 38                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 39                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 40                21  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 41          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 42                -1  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 43                -2  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 44                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 45                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 46  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 47                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 48                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 49                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 50                14  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 51          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 52                -1  1      4160  models.common.Conv                      [128, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 53                -2  1      4160  models.common.Conv                      [128, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 54                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 55                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 56  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 57                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 58                -1  1     73984  models.common.Conv                      [64, 128, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 59          [-1, 47]  1         0  models.common.Concat                    [1]                           \n",
      " 60                -1  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 61                -2  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 62                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 63                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 64  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 65                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 66                -1  1    295424  models.common.Conv                      [128, 256, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 67          [-1, 37]  1         0  models.common.Concat                    [1]                           \n",
      " 68                -1  1     65792  models.common.Conv                      [512, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 69                -2  1     65792  models.common.Conv                      [512, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 70                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 71                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 72  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 73                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 74                57  1     73984  models.common.Conv                      [64, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 75                65  1    295424  models.common.Conv                      [128, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 76                73  1   1180672  models.common.Conv                      [256, 512, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 77      [74, 75, 76]  1     17132  models.yolo.IDetect                     [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 277 layers, 6014988 parameters, 6014988 gradients, 13.2 GFLOPS\n",
      "\n",
      "Fusing layers... \n",
      "IDetect.fuse\n",
      "Model Summary: 277 layers, 6014988 parameters, 6014988 gradients, 13.2 GFLOPS\n",
      "Transferred 330/344 items from yolov7-tiny.pt\n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5233.8\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_LEAKYRELU]: Force to change negative_slope of LeakyReLU from 0.1 to 0.1015625 because DPU only supports this value. It is recommended to change all negative_slope of LeakyReLU to 0.1015625 and re-train the float model for better deployed model accuracy.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(.vai_qat/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5237.1\n",
      "Scaled weight_decay = 0.0005\n",
      "Optimizer groups: 58 .bias, 58 conv.weight, 61 other\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../dataset_mp_nov16/train/labels.cache' images and labels... 11\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../dataset_mp_nov16/valid/labels.cache' images and labels... 1387\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 2.53, Best Possible Recall (BPR) = 1.0000\n",
      "Image sizes 640 train, 640 test\n",
      "Using 8 dataloader workers\n",
      "Logging results to runs/train/yolov7-mp-aug5-tiny-qat2\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      0/99     6.16G   0.06238  0.003876         0   0.06626        52       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5365.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5252.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "\u001b[0;33m[VAIQ_WARN]: The shape of input (torch.Size([3, 384, 672])) should be the same with that of dummy input ([3, 640, 640])\u001b[0m\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  bnfp[1] = stats.mode(data)[0][0]\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816     0.00507     0.00603    8.54e-05    3.06e-05\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      1/99     6.19G   0.04411  0.002451         0   0.04656        29       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5461.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5438.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816     0.00913      0.0325    0.000706    0.000114\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      2/99     6.23G   0.02158  0.001899         0   0.02348        29       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5468.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5268.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.133       0.292      0.0615     0.00994\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      3/99     6.23G   0.01734  0.001676         0   0.01902        51       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5378.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5498.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.227       0.528       0.187      0.0375\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      4/99     6.23G   0.01591   0.00169         0    0.0176        35       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5427.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5090.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.315       0.611       0.323      0.0748\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      5/99     6.23G   0.01508  0.001837         0   0.01692        48       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5203.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5407.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816        0.37       0.571        0.35      0.0805\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      6/99     6.23G   0.01512  0.001879         0     0.017        64       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5409.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5485.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.522       0.741       0.558       0.137\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      7/99     6.23G   0.01454  0.001887         0   0.01642        55       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5435.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5478.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.512       0.658        0.51       0.121\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      8/99     6.23G   0.01387   0.00188         0   0.01575        35       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5452.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5499.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.648       0.725       0.667       0.194\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      9/99     6.23G   0.01356  0.001905         0   0.01547        40       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5360.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5347.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.728       0.746       0.763        0.24\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     10/99     6.23G   0.01316  0.001926         0   0.01509        61       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5439.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1563.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816        0.65       0.777       0.703       0.209\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     11/99     6.23G   0.01343  0.001932         0   0.01536        46       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5399.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5403.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.642       0.795       0.716        0.21\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     12/99     6.23G   0.01296  0.001906         0   0.01487        53       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5342.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5483.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.754       0.733       0.777       0.254\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     13/99     6.23G   0.01273  0.001918         0   0.01465        44       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5384.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5436.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.774       0.819       0.846       0.293\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     14/99     6.23G   0.01269  0.001891         0   0.01458        40       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5315.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5252.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.647       0.706       0.699       0.233\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     15/99     6.23G   0.01259  0.001922         0   0.01451        29       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5409.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5289.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.737       0.704       0.762       0.261\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     16/99     6.23G   0.01279  0.001898         0   0.01469        44       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5375.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5410.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.692       0.669       0.694       0.222\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     17/99     6.23G   0.01231  0.001917         0   0.01423        49       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5280.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1521.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.801        0.77       0.833       0.318\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     18/99     6.23G   0.01279  0.001896         0   0.01469        45       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5419.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5453.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.862       0.819       0.887       0.371\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     19/99     6.23G   0.01245  0.001891         0   0.01434        46       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5444.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5436.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.775       0.726       0.794       0.291\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     20/99     6.23G   0.01213  0.001895         0   0.01403        62       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5456.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5469.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.903       0.882       0.933       0.415\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     21/99     6.23G   0.01236  0.001928         0   0.01429        73       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5386.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5413.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.845       0.833       0.893       0.369\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     22/99     6.23G   0.01232  0.001889         0   0.01421        52       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5420.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5450.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.848       0.816       0.889       0.367\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     23/99     6.23G   0.01229  0.001895         0   0.01418        68       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5468.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5446.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.841       0.892       0.912       0.361\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     24/99     6.23G   0.01206   0.00189         0   0.01395        51       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5473.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1464.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.896       0.881       0.929       0.426\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     25/99     6.23G   0.01248  0.001894         0   0.01437        31       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5432.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5418.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.831       0.854       0.897       0.371\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     26/99     6.23G   0.01208  0.001882         0   0.01396        42       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5401.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5419.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.828        0.87       0.901       0.364\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     27/99     6.23G   0.01238   0.00186         0   0.01424        49       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5407.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5470.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.843       0.872        0.91       0.385\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     28/99     6.23G    0.0119  0.001884         0   0.01379        51       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5425.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5423.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.827       0.865       0.902       0.372\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     29/99     6.23G   0.01166  0.001839         0    0.0135        54       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5371.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5517.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.826        0.85       0.892       0.364\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     30/99     6.23G   0.01188  0.001848         0   0.01372        31       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5456.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5280.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.833       0.871       0.904       0.382\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     31/99     6.23G   0.01169  0.001845         0   0.01353        51       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5450.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5096.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.845       0.875       0.909       0.396\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     32/99     6.23G   0.01173  0.001817         0   0.01355        54       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5310.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1435.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.849       0.881       0.915       0.398\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     33/99     6.23G   0.01173  0.001853         0   0.01358        25       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5469.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5450.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.862       0.861       0.908       0.398\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     34/99     6.23G   0.01172   0.00185         0   0.01357        32       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5281.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5423.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.858       0.866       0.913       0.393\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     35/99     6.23G    0.0114  0.001854         0   0.01326        38       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5432.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5351.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.828       0.894       0.912       0.384\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     36/99     6.23G   0.01173  0.001837         0   0.01356        53       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5245.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5461.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816        0.84       0.868       0.902       0.391\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     37/99     6.23G   0.01176  0.001861         0   0.01362        48       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5248.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5396.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.873       0.875       0.919        0.42\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     38/99     6.23G   0.01158  0.001855         0   0.01344        40       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5443.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1384.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.863       0.887       0.918       0.412\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     39/99     6.23G   0.01191  0.001824         0   0.01373        41       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5391.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5425.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.839       0.901       0.918       0.416\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     40/99     6.23G   0.01155  0.001845         0    0.0134        25       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5398.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5441.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.829       0.892       0.905       0.392\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     41/99     6.23G     0.012  0.001847         0   0.01385        35       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5366.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5446.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.856       0.871       0.908       0.405\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     42/99     6.23G   0.01149  0.001845         0   0.01333        30       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5412.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1348.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.848        0.89       0.915       0.402\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     43/99     6.23G   0.01205  0.001817         0   0.01387        45       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5476.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5211.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.866       0.854       0.905       0.394\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     44/99     6.23G   0.01205  0.001841         0    0.0139        34       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5381.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5365.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.843       0.861         0.9       0.376\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     45/99     6.23G   0.01192  0.001828         0   0.01375        48       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5296.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5242.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.859       0.863       0.906       0.391\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     46/99     6.23G   0.01192  0.001827         0   0.01375        31       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5424.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5332.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.857       0.873       0.906       0.399\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     47/99     6.23G   0.01125  0.001812         0   0.01306        43       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5381.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5516.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.855       0.886       0.911       0.404\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     48/99     6.23G   0.01136  0.001828         0   0.01319        50       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5425.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5346.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.831       0.887       0.908       0.394\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     49/99     6.23G   0.01136  0.001856         0   0.01322        47       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5415.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5359.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.858       0.893       0.918       0.413\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     50/99     6.23G   0.01182  0.001839         0   0.01366        95       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5301.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1329.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.853       0.895       0.909       0.411\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     51/99     6.23G    0.0115  0.001845         0   0.01334        46       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5387.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1273.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.851       0.885       0.906       0.408\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     52/99     6.23G   0.01119  0.001834         0   0.01302        67       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5399.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5328.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.858       0.866       0.899         0.4\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     53/99     6.23G   0.01168  0.001827         0   0.01351        53       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5540.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5333.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.848        0.89       0.909       0.404\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     54/99     6.23G   0.01163  0.001822         0   0.01345        58       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5372.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1274.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.869       0.868       0.904       0.402\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     55/99     6.23G    0.0117  0.001822         0   0.01352        44       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5454.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1272.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.859       0.885       0.912       0.405\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     56/99     6.23G   0.01161  0.001826         0   0.01344        39       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5331.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5121.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.864       0.871       0.902       0.389\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     57/99     6.23G   0.01127  0.001824         0   0.01309        59       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5346.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5213.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.882       0.871        0.91       0.406\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     58/99     6.23G   0.01124  0.001813         0   0.01305        73       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5230.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5416.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.884       0.872       0.919       0.413\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     59/99     6.23G    0.0115  0.001826         0   0.01332        52       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5464.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5331.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.866       0.873       0.903       0.401\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     60/99     6.23G   0.01141  0.001847         0   0.01326        49       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5368.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5410.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.873       0.878       0.917       0.416\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     61/99     6.23G   0.01067  0.001818         0   0.01249        40       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5369.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5511.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816        0.87       0.884       0.916        0.41\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     62/99     6.23G    0.0112   0.00181         0   0.01301        69       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5453.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5471.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.858       0.893       0.919       0.405\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     63/99     6.23G   0.01129  0.001778         0   0.01307        64       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5361.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5214.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.883       0.872       0.914       0.407\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     64/99     6.23G   0.01203  0.001808         0   0.01384        77       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5349.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5289.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.846       0.903       0.911       0.413\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     65/99     6.23G   0.01142  0.001802         0   0.01322        28       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5387.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5403.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.886       0.866       0.913       0.407\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     66/99     6.23G    0.0111  0.001819         0   0.01292        36       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5290.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5291.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.871        0.88       0.911       0.408\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     67/99     6.23G   0.01154  0.001794         0   0.01333        44       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5361.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5214.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.896       0.866       0.919       0.416\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     68/99     6.23G   0.01121  0.001826         0   0.01304        42       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1261.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5430.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816        0.86       0.899       0.917       0.415\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     69/99     6.23G   0.01117  0.001818         0   0.01299        63       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5383.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5294.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.873        0.88        0.91        0.41\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     70/99     6.23G    0.0113  0.001784         0   0.01308        36       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5365.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5397.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.872       0.876       0.908       0.409\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     71/99     6.23G    0.0115  0.001806         0   0.01331        39       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5359.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5488.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.852       0.895       0.907       0.405\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     72/99     6.23G   0.01096  0.001794         0   0.01275        57       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5435.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5374.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.883       0.868       0.915       0.414\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     73/99     6.23G   0.01132  0.001813         0   0.01313        69       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5442.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5432.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.872       0.889       0.921       0.414\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     74/99     6.23G   0.01152  0.001823         0   0.01334        36       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5351.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5262.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.868       0.889       0.921        0.42\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     75/99     6.23G   0.01111  0.001808         0   0.01292        65       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5300.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5354.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.873        0.88       0.919       0.417\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     76/99     6.23G   0.01109  0.001787         0   0.01288        63       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5252.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1163.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.876       0.883       0.919       0.422\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     77/99     6.23G    0.0112  0.001821         0   0.01302        57       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5368.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5325.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.857       0.897       0.917        0.42\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     78/99     6.23G   0.01095  0.001808         0   0.01276        47       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5338.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5297.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.875       0.886        0.92       0.423\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     79/99     6.23G   0.01097  0.001805         0   0.01277        54       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5440.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5380.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.861       0.889       0.919       0.424\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     80/99     6.23G   0.01132  0.001802         0   0.01312        52       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5289.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5174.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816        0.88       0.869       0.917       0.424\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     81/99     6.23G   0.01139  0.001794         0   0.01319        29       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1151.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5357.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816        0.86       0.886       0.915       0.424\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     82/99     6.23G   0.01151  0.001796         0   0.01331        55       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5388.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5327.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.879       0.871       0.919       0.424\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     83/99     6.23G    0.0113  0.001788         0   0.01309        51       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5378.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5384.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.863        0.89       0.918       0.421\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     84/99     6.23G    0.0113  0.001798         0    0.0131        31       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5378.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5351.2\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.858       0.897       0.916       0.423\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     85/99     6.23G   0.01139  0.001807         0   0.01319        49       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5256.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5126.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.869       0.881       0.917       0.422\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     86/99     6.23G   0.01162  0.001786         0    0.0134        48       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5336.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1098.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.858       0.893       0.913        0.42\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     87/99     6.23G   0.01155  0.001818         0   0.01337        46       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5339.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5297.5\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.873       0.884        0.92       0.424\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     88/99     6.23G   0.01127  0.001813         0   0.01308        49       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5374.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5307.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.877       0.882       0.919       0.423\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     89/99     6.23G   0.01098  0.001785         0   0.01276        48       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5345.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5288.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.862       0.895       0.918       0.421\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     90/99     6.23G   0.01165  0.001797         0   0.01344        66       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5308.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5323.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.865       0.895       0.919       0.427\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     91/99     6.23G   0.01122  0.001825         0   0.01304        44       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5399.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5292.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.874       0.881       0.918       0.424\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     92/99     6.23G   0.01128  0.001816         0    0.0131        63       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5368.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5379.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.871       0.876       0.915       0.422\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     93/99     6.23G   0.01162  0.001821         0   0.01344        45       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5349.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5350.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.858       0.899       0.917        0.42\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     94/99     6.23G   0.01129  0.001781         0   0.01307        41       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5353.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1060.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816        0.85         0.9       0.916       0.417\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     95/99     6.23G    0.0113  0.001803         0    0.0131        43       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5367.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5281.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.846       0.899       0.911       0.414\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     96/99     6.23G   0.01119  0.001805         0     0.013        40       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5386.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5279.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.853       0.891       0.915       0.415\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     97/99     6.23G    0.0111  0.001814         0   0.01291        20       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5348.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5264.3\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.856       0.891       0.911       0.412\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     98/99     6.23G     0.011  0.001795         0   0.01279        43       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5252.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5223.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.857       0.892       0.913        0.41\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "     99/99     6.23G   0.01131  0.001829         0   0.01313        53       640\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5327.7\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5198.1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "                 all        1387        3816       0.854       0.889       0.908       0.411\n",
      "100 epochs completed in 7.633 hours.\n",
      "\n",
      "Optimizer stripped from runs/train/yolov7-mp-aug5-tiny-qat2/weights/last.pt, 61.0MB\n",
      "Optimizer stripped from runs/train/yolov7-mp-aug5-tiny-qat2/weights/best.pt, 61.0MB\n",
      "/workspace/yolov7\n"
     ]
    }
   ],
   "source": [
    "%cd yolov7/\n",
    "!python train_qat.py --workers 16 --device 0 --epochs 100 --batch-size 8 --data data/mp.yaml --img 640 640 --cfg cfg/training/yolov7-tiny.yaml --weights yolov7-tiny.pt --name yolov7-mp-aug5-tiny-qat --hyp data/hyp.scratch.p5_qat.yaml --nndct_convert_sigmoid_to_hsigmoid --nndct_convert_silu_to_hswishv \n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23ef95db-ce67-45d6-a03e-048f8bec9045",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/yolov7/yolov7\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/posixpath.py\", line 177, in lexists\n",
      "    os.lstat(path)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './coco/images/test2017/cfg'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"train_qat.py\", line 646, in <module>\n",
      "    opt.data, opt.cfg, opt.hyp = check_file(opt.data), check_file(opt.cfg), check_file(opt.hyp)  # check files\n",
      "  File \"/workspace/yolov7/yolov7/utils/general.py\", line 150, in check_file\n",
      "    files = glob.glob('./**/' + file, recursive=True)  # find file\n",
      "  File \"/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/glob.py\", line 21, in glob\n",
      "    return list(iglob(pathname, recursive=recursive))\n",
      "  File \"/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/glob.py\", line 73, in _iglob\n",
      "    for dirname in dirs:\n",
      "  File \"/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/glob.py\", line 73, in _iglob\n",
      "    for dirname in dirs:\n",
      "  File \"/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/glob.py\", line 74, in _iglob\n",
      "    for name in glob_in_dir(dirname, basename, dironly):\n",
      "  File \"/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/glob.py\", line 94, in _glob0\n",
      "    if os.path.lexists(os.path.join(dirname, basename)):\n",
      "  File \"/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/posixpath.py\", line 177, in lexists\n",
      "    os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "/workspace/yolov7\n"
     ]
    }
   ],
   "source": [
    "%cd yolov7/\n",
    "!python train_qat.py --workers 16 --device 0 --epochs 100 --batch-size 8 --data data/mp.yaml --img 640 640 --cfg cfg/training/yolov7.yaml --weights yolov7_training.pt --name yolov7_mp- --hyp data/hyp.scratch.p5_qat.yaml --nndct_convert_sigmoid_to_hsigmoid --nndct_convert_silu_to_hswishv \n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72820707-5e27-438a-86d6-419fed24bf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m569.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.27.0,>=1.19.5 (from scipy)\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.4\n",
      "    Uninstalling numpy-1.23.4:\n",
      "      Successfully uninstalled numpy-1.23.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytorch-nndct 3.5.0 requires ninja, which is not installed.\n",
      "pytorch-nndct 3.5.0 requires numpy<=1.24.2, but you have numpy 1.24.4 which is incompatible.\n",
      "pytorch-nndct 3.5.0 requires scipy<=1.9.3, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.4 scipy-1.10.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U --force-reinstall scipy\n",
    "it did not help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29480b8b",
   "metadata": {},
   "source": [
    "## Eval QAT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c42ece",
   "metadata": {},
   "source": [
    "### run quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c70fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SHIFT_CHECK]: bias Model::model.102.rbr_reparam.bias value is too small, so adjust the fix position from 26 to 20\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SHIFT_CHECK]: bias Model::model.103.rbr_reparam.bias value is too small, so adjust the fix position from 26 to 21\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SHIFT_CHECK]: bias Model::model.104.rbr_reparam.bias value is too small, so adjust the fix position from 26 to 21\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.105.ia.0.implicit\n",
      "freezing model.105.ia.1.implicit\n",
      "freezing model.105.ia.2.implicit\n",
      "freezing model.105.im.0.implicit\n",
      "freezing model.105.im.1.implicit\n",
      "freezing model.105.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- xcdl190200\n",
      "              release --- 4.4.0-134-generic\n",
      "              version --- #160-Ubuntu SMP Wed Aug 15 14:58:00 UTC 2018\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 9.4.0\n",
      "               python --- 3.7.12\n",
      "              pytorch --- 1.12.1\n",
      "        vai_q_pytorch --- 3.0.0+1bbeff9+torch1.12.1\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- Tesla P100-PCIE-16GB\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_REPLACE_SILU]: SiLU has been replaced by Hardswish.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model Model is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 2358.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'coco/val2017.cache' images and labels... 4952 found, 48 missing, \u001b[0m\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.7/site-packages/torch/onnx/utils.py:1523: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input Model::input_0\n",
      "  key\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.7/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.7/site-packages/pytorch_nndct/quantization/torchquantizer.py:55: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if inf.sum() > 0 or nan.sum() > 0:\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.7/site-packages/pytorch_nndct/nn/modules/fix_ops.py:69: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  tensor.storage().size() != tensor.numel()):\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Model_int.onnx is generated.(nndct/Model_int.onnx)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Model_int.pt is generated.(nndct/Model_int.pt)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Dumping 'Model_0'' checking data...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Finsh dumping data.(nndct/deploy_check_data_int/Model_0)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Successfully convert 'Model_0' to xmodel.(nndct/Model_0_int.xmodel)\u001b[0m\n",
      "                 all        5000       36335       0.739       0.614        0.67       0.463\n",
      "Speed: 104.1/1.1/105.2 ms inference/NMS/total per 640x640 image at batch-size 1\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/test/yolov7_640_val5/qat_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=4.16s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=57.16s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=10.51s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.677\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.526\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.323\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.516\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.626\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.366\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.603\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.655\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.498\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.698\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.803\n",
      "Results saved to runs/test/yolov7_640_val5\n",
      "/workspace/internal-models/pytorch/yolov7\n"
     ]
    }
   ],
   "source": [
    "%cd yolov7/\n",
    "!python test_nndct.py --data data/coco.yaml --img 640 --batch 1 --conf 0.001 --iou 0.65 --device 0 --weights ../quantized/qat_09.pt --name yolov7_640_val --quant_mode test --nndct_qat --nndct_convert_sigmoid_to_hsigmoid --nndct_convert_silu_to_hswish\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499b0a1",
   "metadata": {},
   "source": [
    "### dump QAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f807fd-73e3-44f5-9cf7-9725f86b4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpy QAT yolov7 tiny model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "867a8c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/yolov7/yolov7\n",
      "Namespace(augment=False, batch_size=8, conf_thres=0.001, data='data/mp.yaml', device='0', exist_ok=False, img_size=640, iou_thres=0.65, name='yolov_tiny_640_val', nndct_qat=True, no_trace=False, project='runs/test', save_conf=False, save_hybrid=False, save_json=False, save_txt=False, single_cls=False, task='val', v5_metric=False, verbose=False, weights=['./qat_tiny/yolov7-mp-aug5-tiny-qat2/weights/last.pt'])\n",
      "YOLOR ðŸš€ v3.5-19-gb2b227921 torch 1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3090 Ti, 24239.5625MB)\n",
      "\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "Fusing layers... \n",
      "IDetect.fuse\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 277 layers, 6014988 parameters, 0 gradients, 13.2 GFLOPS\n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1956.8\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_LEAKYRELU]: Force to change negative_slope of LeakyReLU from 0.1 to 0.1015625 because DPU only supports this value. It is recommended to change all negative_slope of LeakyReLU to 0.1015625 and re-train the float model for better deployed model accuracy.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(.vai_qat/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5320.8\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5482.4\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.0.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.0.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.1.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.1.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.2.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.2.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.3.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.3.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.4.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.4.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.5.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.5.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.7.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.7.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.9.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.9.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.10.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.10.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.11.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.11.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.12.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.12.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.14.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.14.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.16.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.16.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.17.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.17.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.18.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.18.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.19.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.19.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.21.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.21.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.23.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.23.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.24.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.24.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.25.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.25.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.26.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.26.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.28.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.28.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.29.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.29.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.30.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.30.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.35.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.35.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.37.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.37.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.38.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.38.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.40.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.40.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.42.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.42.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.43.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.43.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.44.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.44.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.45.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.45.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.47.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.47.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.48.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.48.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.50.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.50.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.52.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.52.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.53.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.53.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.54.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.54.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.55.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.55.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.57.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.57.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.58.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.58.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.60.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.60.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.61.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.61.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.62.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.62.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.63.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.63.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.65.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.65.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.66.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.66.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.68.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.68.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.69.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.69.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.70.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.70.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.71.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.71.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.73.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.73.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.74.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.74.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.75.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.75.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.76.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.76.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.77.m.0.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.77.m.0.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.77.m.1.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.77.m.1.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.77.m.2.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.77.m.2.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/QuantStub[quant]/8660.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[0]/LeakyReLU[act]/8692.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[1]/LeakyReLU[act]/8721.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[6]/Cat[cat]/ret.29.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[3]/LeakyReLU[act]/8779.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[4]/LeakyReLU[act]/8808.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[7]/LeakyReLU[act]/8870.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/MP[model]/MP[8]/MaxPool2d[m]/8885.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[13]/Cat[cat]/ret.51.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[10]/LeakyReLU[act]/8944.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[11]/LeakyReLU[act]/8973.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[14]/LeakyReLU[act]/9035.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/MP[model]/MP[15]/MaxPool2d[m]/9050.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[20]/Cat[cat]/ret.73.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[17]/LeakyReLU[act]/9109.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[18]/LeakyReLU[act]/9138.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[21]/LeakyReLU[act]/9200.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/MP[model]/MP[22]/MaxPool2d[m]/9215.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[27]/Cat[cat]/ret.95.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[24]/LeakyReLU[act]/9274.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[25]/LeakyReLU[act]/9303.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[28]/LeakyReLU[act]/9365.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[36]/Cat[cat]/ret.115.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[30]/LeakyReLU[act]/9423.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[34]/Cat[cat]/ret.109.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[37]/LeakyReLU[act]/9537.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[38]/LeakyReLU[act]/9566.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[41]/Cat[cat]/ret.131.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[46]/Cat[cat]/ret.149.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[43]/LeakyReLU[act]/9663.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[44]/LeakyReLU[act]/9692.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[47]/LeakyReLU[act]/9754.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[48]/LeakyReLU[act]/9783.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[51]/Cat[cat]/ret.165.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[56]/Cat[cat]/ret.183.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[53]/LeakyReLU[act]/9880.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[54]/LeakyReLU[act]/9909.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[57]/LeakyReLU[act]/9971.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[59]/Cat[cat]/ret.193.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[64]/Cat[cat]/ret.211.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[61]/LeakyReLU[act]/10062.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[62]/LeakyReLU[act]/10091.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[65]/LeakyReLU[act]/10153.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[67]/Cat[cat]/ret.221.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[72]/Cat[cat]/ret.239.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[69]/LeakyReLU[act]/10244.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[70]/LeakyReLU[act]/10273.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[73]/LeakyReLU[act]/10335.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[74]/LeakyReLU[act]/10364.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[75]/LeakyReLU[act]/10393.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[76]/LeakyReLU[act]/10422.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/NNDctDetect[model]/NNDctDetect[77]/Conv2d[m]/ModuleList[0]/ret.257.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/NNDctDetect[model]/NNDctDetect[77]/Conv2d[m]/ModuleList[1]/ret.261.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/NNDctDetect[model]/NNDctDetect[77]/Conv2d[m]/ModuleList[2]/ret.265.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.77.ia.0.implicit\n",
      "freezing model.77.ia.1.implicit\n",
      "freezing model.77.ia.2.implicit\n",
      "freezing model.77.im.0.implicit\n",
      "freezing model.77.im.1.implicit\n",
      "freezing model.77.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 5434.9\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../dataset_mp_nov16/valid/labels.cache' images and labels... 1387\u001b[0m\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input Model::input_0\n",
      "  warnings.warn(\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if inf.sum() > 0 or nan.sum() > 0:\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (isinstance(tensor, torch.Tensor) and\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Model_int.onnx is generated.(nndct/Model_int.onnx)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Model_int.pt is generated.(nndct/Model_int.pt)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Dumping 'Model_0'' checking data...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Finish dumping data.(nndct/deploy_check_data_int/Model_0)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Successfully convert 'Model_0' to xmodel.(nndct/Model_0_int.xmodel)\u001b[0m\n",
      "/workspace/yolov7\n"
     ]
    }
   ],
   "source": [
    "# tiny\n",
    "%cd yolov7/\n",
    "!python test_nndct.py --data data/mp.yaml --img 640 --batch 8 --conf 0.001 --iou 0.65 --device 0 --weights ./qat_tiny/yolov7-mp-aug5-tiny-qat2/weights/last.pt --name yolov_tiny_640_val --quant_mode test --nndct_qat --nndct_convert_sigmoid_to_hsigmoid --nndct_convert_silu_to_hswish --dump_model\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc2cc6-1197-4625-be7d-cfb210f8a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump qat yolov7 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1204f7ec-8e5c-4d13-af5a-cbb83fe7003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/yolov7/yolov7\n",
      "Namespace(augment=False, batch_size=8, conf_thres=0.001, data='data/mp.yaml', device='0', exist_ok=False, img_size=640, iou_thres=0.65, name='yolov7_640_val', nndct_qat=True, no_trace=False, project='runs/test', save_conf=False, save_hybrid=False, save_json=False, save_txt=False, single_cls=False, task='val', v5_metric=False, verbose=False, weights=['./qat_yolov7/yolov7_mp_aug_3_qat_yolov7/weights/last.pt'])\n",
      "YOLOR ðŸš€ v3.5-19-gb2b227921 torch 1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3090 Ti, 24239.5625MB)\n",
      "\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "Fusing layers... \n",
      "IDetect.fuse\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 417 layers, 36503052 parameters, 0 gradients, 103.8 GFLOPS\n",
      "freezing model.105.ia.0.implicit\n",
      "freezing model.105.ia.1.implicit\n",
      "freezing model.105.ia.2.implicit\n",
      "freezing model.105.im.0.implicit\n",
      "freezing model.105.im.1.implicit\n",
      "freezing model.105.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_REPLACE_SILU]: SiLU has been replaced by Hardswish.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 4749.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(.vai_qat/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 2515.6\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_DEPRECATED_ARGUMENT]: \"convert_to_deployable\" is deprecated and will be removed in the future. Use \"to_deployable\" instead.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 4963.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(.vai_qat/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.0.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.0.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.1.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.1.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.2.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.2.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.3.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.3.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.4.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.4.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.5.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.5.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.6.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.6.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.7.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.7.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.8.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.8.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.9.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.9.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.11.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.11.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.13.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.13.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.14.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.14.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.15.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.15.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.17.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.17.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.18.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.18.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.19.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.19.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.20.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.20.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.21.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.21.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.22.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.22.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.24.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.24.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.26.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.26.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.27.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.27.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.28.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.28.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.30.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.30.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.31.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.31.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.32.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.32.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.33.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.33.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.34.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.34.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.35.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.35.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.37.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.37.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.39.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.39.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.40.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.40.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.41.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.41.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.43.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.43.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.44.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.44.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.45.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.45.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.46.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.46.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.47.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.47.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.48.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.48.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.50.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.50.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv1.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv1.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv3.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv3.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv4.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv4.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv5.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv5.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv6.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv6.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv2.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv2.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv7.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.51.cv7.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.52.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.52.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.54.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.54.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.56.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.56.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.57.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.57.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.58.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.58.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.59.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.59.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.60.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.60.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.61.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.61.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.63.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.63.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.64.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.64.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.66.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.66.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.68.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.68.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.69.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.69.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.70.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.70.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.71.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.71.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.72.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.72.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.73.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.73.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.75.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.75.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.77.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.77.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.78.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.78.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.79.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.79.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.81.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.81.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.82.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.82.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.83.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.83.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.84.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.84.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.85.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.85.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.86.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.86.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.88.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.88.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.90.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.90.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.91.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.91.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.92.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.92.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.94.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.94.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.95.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.95.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.96.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.96.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.97.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.97.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.98.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.98.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.99.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.99.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.101.conv.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.101.conv.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.102.rbr_reparam.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.102.rbr_reparam.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.103.rbr_reparam.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.103.rbr_reparam.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.104.rbr_reparam.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.104.rbr_reparam.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.105.m.0.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.105.m.0.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.105.m.1.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.105.m.1.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.105.m.2.weight.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::model.105.m.2.bias.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/QuantStub[quant]/13614.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[0]/Conv2d[conv]/ret.5.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[0]/Hardswish[act]/ret.9.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[1]/Conv2d[conv]/ret.11.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[1]/Hardswish[act]/ret.15.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[2]/Conv2d[conv]/ret.17.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[2]/Hardswish[act]/ret.21.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[3]/Conv2d[conv]/ret.23.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[3]/Hardswish[act]/ret.27.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[4]/Conv2d[conv]/ret.29.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[10]/Cat[cat]/ret.65.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[5]/Conv2d[conv]/ret.35.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[5]/Hardswish[act]/ret.39.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[6]/Conv2d[conv]/ret.41.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[6]/Hardswish[act]/ret.45.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[7]/Conv2d[conv]/ret.47.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[7]/Hardswish[act]/ret.51.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[8]/Conv2d[conv]/ret.53.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[8]/Hardswish[act]/ret.57.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[9]/Conv2d[conv]/ret.59.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[11]/Conv2d[conv]/ret.67.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[11]/Hardswish[act]/ret.71.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/MP[model]/MP[12]/MaxPool2d[m]/13955.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[13]/Conv2d[conv]/ret.73.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[16]/Cat[cat]/ret.91.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[14]/Conv2d[conv]/ret.79.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[14]/Hardswish[act]/ret.83.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[15]/Conv2d[conv]/ret.85.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[17]/Conv2d[conv]/ret.93.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[23]/Cat[cat]/ret.129.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[18]/Conv2d[conv]/ret.99.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[18]/Hardswish[act]/ret.103.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[19]/Conv2d[conv]/ret.105.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[19]/Hardswish[act]/ret.109.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[20]/Conv2d[conv]/ret.111.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[20]/Hardswish[act]/ret.115.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[21]/Conv2d[conv]/ret.117.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[21]/Hardswish[act]/ret.121.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[22]/Conv2d[conv]/ret.123.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[24]/Conv2d[conv]/ret.131.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[24]/Hardswish[act]/ret.135.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/MP[model]/MP[25]/MaxPool2d[m]/14269.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[26]/Conv2d[conv]/ret.137.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[29]/Cat[cat]/ret.155.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[27]/Conv2d[conv]/ret.143.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[27]/Hardswish[act]/ret.147.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[28]/Conv2d[conv]/ret.149.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[30]/Conv2d[conv]/ret.157.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[36]/Cat[cat]/ret.193.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[31]/Conv2d[conv]/ret.163.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[31]/Hardswish[act]/ret.167.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[32]/Conv2d[conv]/ret.169.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[32]/Hardswish[act]/ret.173.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[33]/Conv2d[conv]/ret.175.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[33]/Hardswish[act]/ret.179.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[34]/Conv2d[conv]/ret.181.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[34]/Hardswish[act]/ret.185.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[35]/Conv2d[conv]/ret.187.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[37]/Conv2d[conv]/ret.195.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[37]/Hardswish[act]/ret.199.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/MP[model]/MP[38]/MaxPool2d[m]/14583.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[39]/Conv2d[conv]/ret.201.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[42]/Cat[cat]/ret.219.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[40]/Conv2d[conv]/ret.207.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[40]/Hardswish[act]/ret.211.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[41]/Conv2d[conv]/ret.213.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[43]/Conv2d[conv]/ret.221.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[49]/Cat[cat]/ret.257.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[44]/Conv2d[conv]/ret.227.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[44]/Hardswish[act]/ret.231.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[45]/Conv2d[conv]/ret.233.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[45]/Hardswish[act]/ret.237.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[46]/Conv2d[conv]/ret.239.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[46]/Hardswish[act]/ret.243.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[47]/Conv2d[conv]/ret.245.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[47]/Hardswish[act]/ret.249.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[48]/Conv2d[conv]/ret.251.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[50]/Conv2d[conv]/ret.259.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[50]/Hardswish[act]/ret.263.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv1]/Conv2d[conv]/ret.265.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv1]/Hardswish[act]/ret.269.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv3]/Conv2d[conv]/ret.271.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv3]/Hardswish[act]/ret.275.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv4]/Conv2d[conv]/ret.277.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv4]/Hardswish[act]/ret.281.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Cat[cat]/ret.283.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv5]/Conv2d[conv]/ret.285.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv5]/Hardswish[act]/ret.289.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv6]/Conv2d[conv]/ret.291.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Cat[cat2]/ret.303.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv2]/Conv2d[conv]/ret.297.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv7]/Conv2d[conv]/ret.305.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/SPPCSPC[model]/SPPCSPC[51]/Conv[cv7]/Hardswish[act]/ret.309.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[52]/Conv2d[conv]/ret.311.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[52]/Hardswish[act]/ret.315.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[55]/Cat[cat]/ret.325.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[54]/Conv2d[conv]/ret.319.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[56]/Conv2d[conv]/ret.327.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[62]/Cat[cat]/ret.363.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[57]/Conv2d[conv]/ret.333.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[57]/Hardswish[act]/ret.337.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[58]/Conv2d[conv]/ret.339.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[58]/Hardswish[act]/ret.343.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[59]/Conv2d[conv]/ret.345.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[59]/Hardswish[act]/ret.349.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[60]/Conv2d[conv]/ret.351.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[60]/Hardswish[act]/ret.355.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[61]/Conv2d[conv]/ret.357.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[63]/Conv2d[conv]/ret.365.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[63]/Hardswish[act]/ret.369.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[64]/Conv2d[conv]/ret.371.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[64]/Hardswish[act]/ret.375.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[67]/Cat[cat]/ret.385.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[66]/Conv2d[conv]/ret.379.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[68]/Conv2d[conv]/ret.387.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[74]/Cat[cat]/ret.423.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[69]/Conv2d[conv]/ret.393.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[69]/Hardswish[act]/ret.397.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[70]/Conv2d[conv]/ret.399.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[70]/Hardswish[act]/ret.403.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[71]/Conv2d[conv]/ret.405.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[71]/Hardswish[act]/ret.409.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[72]/Conv2d[conv]/ret.411.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[72]/Hardswish[act]/ret.415.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[73]/Conv2d[conv]/ret.417.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[75]/Conv2d[conv]/ret.425.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[75]/Hardswish[act]/ret.429.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/MP[model]/MP[76]/MaxPool2d[m]/15706.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[77]/Conv2d[conv]/ret.431.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[80]/Cat[cat]/ret.449.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[78]/Conv2d[conv]/ret.437.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[78]/Hardswish[act]/ret.441.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[79]/Conv2d[conv]/ret.443.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[81]/Conv2d[conv]/ret.451.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[87]/Cat[cat]/ret.487.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[82]/Conv2d[conv]/ret.457.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[82]/Hardswish[act]/ret.461.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[83]/Conv2d[conv]/ret.463.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[83]/Hardswish[act]/ret.467.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[84]/Conv2d[conv]/ret.469.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[84]/Hardswish[act]/ret.473.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[85]/Conv2d[conv]/ret.475.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[85]/Hardswish[act]/ret.479.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[86]/Conv2d[conv]/ret.481.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[88]/Conv2d[conv]/ret.489.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[88]/Hardswish[act]/ret.493.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/MP[model]/MP[89]/MaxPool2d[m]/16020.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[90]/Conv2d[conv]/ret.495.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[93]/Cat[cat]/ret.513.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[91]/Conv2d[conv]/ret.501.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[91]/Hardswish[act]/ret.505.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[92]/Conv2d[conv]/ret.507.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[94]/Conv2d[conv]/ret.515.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Concat[model]/Concat[100]/Cat[cat]/ret.551.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[95]/Conv2d[conv]/ret.521.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[95]/Hardswish[act]/ret.525.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[96]/Conv2d[conv]/ret.527.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[96]/Hardswish[act]/ret.531.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[97]/Conv2d[conv]/ret.533.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[97]/Hardswish[act]/ret.537.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[98]/Conv2d[conv]/ret.539.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[98]/Hardswish[act]/ret.543.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[99]/Conv2d[conv]/ret.545.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[101]/Conv2d[conv]/ret.553.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/Conv[model]/Conv[101]/Hardswish[act]/ret.557.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/RepConv[model]/RepConv[102]/Conv2d[rbr_reparam]/ret.559.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/RepConv[model]/RepConv[102]/Hardswish[act]/ret.561.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/RepConv[model]/RepConv[103]/Conv2d[rbr_reparam]/ret.563.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/RepConv[model]/RepConv[103]/Hardswish[act]/ret.565.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/RepConv[model]/RepConv[104]/Conv2d[rbr_reparam]/ret.567.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/RepConv[model]/RepConv[104]/Hardswish[act]/ret.569.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/NNDctDetect[model]/NNDctDetect[105]/Conv2d[m]/ModuleList[0]/ret.571.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/NNDctDetect[model]/NNDctDetect[105]/Conv2d[m]/ModuleList[1]/ret.575.\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SCALE_VALUE]: Exported scale values are not trained: Model::Model/NNDctDetect[model]/NNDctDetect[105]/Conv2d[m]/ModuleList[2]/ret.579.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/test/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(nndct/quant_info.json)\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SHIFT_CHECK]: bias Model::model.102.rbr_reparam.bias value is too small, so adjust the fix position from 26 to 20\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SHIFT_CHECK]: bias Model::model.103.rbr_reparam.bias value is too small, so adjust the fix position from 26 to 21\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_SHIFT_CHECK]: bias Model::model.104.rbr_reparam.bias value is too small, so adjust the fix position from 26 to 21\u001b[0m\n",
      " Convert model to Traced-model... \n",
      "freezing model.105.ia.0.implicit\n",
      "freezing model.105.ia.1.implicit\n",
      "freezing model.105.ia.2.implicit\n",
      "freezing model.105.im.0.implicit\n",
      "freezing model.105.im.1.implicit\n",
      "freezing model.105.im.2.implicit\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "                 node --- baset-MS-7D25\n",
      "              release --- 5.15.0-117-generic\n",
      "              version --- #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 7.5.0\n",
      "               python --- 3.8.6\n",
      "              pytorch --- 1.13.1+cu117\n",
      "        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1+cu117\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- NVIDIA GeForce RTX 3090 Ti\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_REPLACE_SILU]: SiLU has been replaced by Hardswish.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_Model_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 4880.6\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(nndct/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      " model is traced! \n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../dataset_mp_nov16/valid/labels.cache' images and labels... 1387\u001b[0m\n",
      "               Class      Images      Labels           P           R      mAP@.5\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input Model::input_0\n",
      "  warnings.warn(\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if inf.sum() > 0 or nan.sum() > 0:\n",
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (isinstance(tensor, torch.Tensor) and\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Model_int.onnx is generated.(nndct/Model_int.onnx)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Model_int.pt is generated.(nndct/Model_int.pt)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Dumping 'Model_0'' checking data...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Finish dumping data.(nndct/deploy_check_data_int/Model_0)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Successfully convert 'Model_0' to xmodel.(nndct/Model_0_int.xmodel)\u001b[0m\n",
      "/workspace/yolov7\n"
     ]
    }
   ],
   "source": [
    "# yolov7\n",
    "%cd yolov7/yolov7\n",
    "!python test_nndct.py --data data/mp.yaml --img 640 --batch 8 --conf 0.001 --iou 0.65 --device 0 --weights ./qat_yolov7/yolov7_mp_aug_3_qat_yolov7/weights/last.pt --name yolov7_640_val --quant_mode test --nndct_qat --nndct_convert_sigmoid_to_hsigmoid --nndct_convert_silu_to_hswish --dump_model\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b2a10-4105-45c1-a071-fb7d505ad4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd yolov7/\n",
    "!python test_nndct.py --data data/coco.yaml --img 640 --batch 1 --conf 0.001 --iou 0.65 --device 0 --weights ../quantized/qat_09.pt --name yolov7_640_val --quant_mode test --nndct_qat --nndct_convert_sigmoid_to_hsigmoid --nndct_convert_silu_to_hswish --dump_model\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648aeba-347d-4582-9412-2d39823b8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vai_c_xir --xmodel quantized_model/YOLOv5_quantized.xmodel  --arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/KV260/arch.json --net_name yolov5_kv260 --output_dir ./KV260"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34f8af-a6b2-4c5d-96ac-7f6ef206f514",
   "metadata": {},
   "source": [
    "## Performance\n",
    "| Model             | Input Size | Float mAP   | Quant mAP   | QAT mAP   | FLOPs  |\n",
    "|-------------------|------------|-------------|-------------|-------------|--------|\n",
    "| YOLOv7           | 640\\*640   | 50.9%       | 40.8%       | 47.9%       | 104.8G  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
